from typing import List, Iterable
from src.entity.proxy_entity import ProxyEntity
from src.enum.common import ProxyCoverEnum, ProxyTypeEnum
from src.log.logger import logger
from src.spider.abs_spider import AbsSpider
from bs4 import BeautifulSoup, Tag

spider_collection = {}


def spider_register(cls):
    spider_collection.update({cls.__name__: cls()})
    logger.info(f'Ê≥®ÂÜå{cls.__name__}')
    return cls


@spider_register
class Spider66Ip(AbsSpider):
    """
    66IP‰ª£ÁêÜÁà¨Ëô´ Âà∑Êñ∞ÈÄüÂ∫¶:üêåÊÖ¢
    http://www.66ip.cn/
    """

    def __init__(self) -> None:
        super().__init__('66IP‰ª£ÁêÜÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        tr_list = soup.find('table', attrs={'width': '100%', 'bordercolor': '#6699ff'}).find_all('tr')
        for i, tr in enumerate(tr_list):
            if i == 0:
                continue
            contents = tr.contents
            ip = contents[0].text
            port = contents[1].text
            region = contents[2].text
            proxy_cover = contents[3].text
            result.append(
                ProxyEntity(
                    f'http://{ip}:{port}',
                    source=self._name,
                    proxy_cover=self._judge_proxy_cover(proxy_cover),
                    region=region))
        return result

    def get_urls(self) -> List[str]:
        return ['http://www.66ip.cn']

    def get_page_range(self) -> Iterable:
        return range(1, 6)

    def get_page_url(self, url, page) -> str:
        return f'{url}/{page}.html'

    def get_encoding(self) -> str:
        return 'gb2312'

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'È´òÂåø‰ª£ÁêÜ':
            return ProxyCoverEnum.HIGH_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderQuanWangIp(AbsSpider):
    """
    ÂÖ®ÁΩëIP‰ª£ÁêÜÁà¨Ëô´ Âà∑Êñ∞ÈÄüÂ∫¶:ÊûÅÂø´
    http://www.goubanjia.com/
    """

    def __init__(self) -> None:
        super().__init__('ÂÖ®ÁΩëIP‰ª£ÁêÜÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        tr_list = soup.find('tbody').find_all('tr')
        for i, tr in enumerate(tr_list):
            tds = tr.find_all('td')
            id_and_port = tds[0]
            ip, port = self._parse_ip_and_port(id_and_port)
            proxy_cover = tds[1].text
            proxy_type = tds[2].text
            region = tds[3].contents[1].text
            supplier = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      supplier=supplier,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region
                                      )
                          )
        return result

    def get_urls(self) -> List[str]:
        return ['http://www.goubanjia.com']

    def get_page_url(self, url, page) -> str:
        return url

    def _parse_ip_and_port(self, ip_td: Tag):
        res = []
        contents = ip_td.find_all(['div', 'span'])
        for content in contents:
            res.append(content.text)
        res.pop()
        ip = ''.join(res)

        port_tag = contents[-1]
        port_ori_str = port_tag.get('class')[1]
        # Ëß£Á†ÅÁúüÂÆûÁöÑÁ´ØÂè£
        port = 0
        for c in port_ori_str:
            port *= 10
            port += (ord(c) - ord('A'))
        port /= 8
        port = int(port)
        return ip, str(port)

    def _judge_proxy_type(self, type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    def _judge_proxy_cover(self, cover_str: str):
        if cover_str == 'ÈÄèÊòé':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'È´òÂåø':
            return ProxyCoverEnum.HIGH_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderXiciIp(AbsSpider):
    """
    Ë•øÂà∫‰ª£ÁêÜÁà¨Ëô´ Âà∑Êñ∞ÈÄüÂ∫¶:üêåÊÖ¢
    Âü∫Êú¨‰∏äÊ≤°Âá†‰∏™‰ª£ÁêÜ‰∏™ËÉΩÁî®üÜí
    https://www.xicidaili.com/
    """

    def __init__(self) -> None:
        super().__init__('Ë•øÂà∫IP‰ª£ÁêÜÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        tab = soup.find('table', attrs={'id': 'ip_list'})
        if tab is None:
            return []
        tr_list = tab.find_all('tr')[1: -1]
        for tr in tr_list:
            tds = tr.find_all('td')
            ip = tds[1].text
            port = tds[2].text
            proxy_cover = tds[4].text
            proxy_type = tds[5].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      ))
        return result

    def get_urls(self) -> List[str]:
        return [
            'https://www.xicidaili.com/nn',  # È´òÂåø
            'https://www.xicidaili.com/nt'  # ÈÄèÊòé
        ]

    def get_page_range(self) -> Iterable:
        return range(1, 3)

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'È´òÂåø':
            return ProxyCoverEnum.HIGH_COVER.value
        if cover_str == 'ÈÄèÊòé':
            return ProxyCoverEnum.TRANSPARENT.value
        else:
            return ProxyCoverEnum.UNKNOWN.value

    @staticmethod
    def _judge_proxy_type(type_str: str):
        if type_str == 'HTTPS':
            return ProxyTypeEnum.HTTPS.value
        if type_str == 'HTTP':
            return ProxyTypeEnum.HTTP.value
        else:
            return ProxyTypeEnum.UNKNOWN.value


@spider_register
class SpiderKuaiDaiLiIp(AbsSpider):
    """
    Âø´‰ª£ÁêÜIP Âà∑Êñ∞ÈÄüÂ∫¶: ÊûÅÂø´
    https://www.kuaidaili.com/free
    """

    def __init__(self) -> None:
        super().__init__('Âø´‰ª£ÁêÜIP‰ª£ÁêÜÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        trs = soup.find('tbody').find_all('tr')
        for tr in trs:
            # print(tr)
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text
            region = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      # ip, port, protocol=proxy_type.lower(),
                                      source=self._name,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_urls(self) -> List[str]:
        return [
            'https://www.kuaidaili.com/free/inha',  # È´òÂåø
            'https://www.kuaidaili.com/free/intr'  # ÈÄèÊòé
        ]

    def get_page_range(self) -> Iterable:
        return range(1, 5)

    # Áà¨Â§™Âø´‰ºöË¢´Â∞Å
    def get_interval(self) -> int:
        return 3

    def _judge_proxy_type(self, type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    def _judge_proxy_cover(self, cover_str: str):
        if cover_str == 'ÈÄèÊòé':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'È´òÂåøÂêç':
            return ProxyCoverEnum.HIGH_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderYunDaiLiIp(AbsSpider):
    """
    ‰∫ë‰ª£ÁêÜIP Âà∑Êñ∞ÈÄüÂ∫¶: Âø´
    http://www.ip3366.net/free
    """

    def __init__(self) -> None:
        super().__init__('‰∫ë‰ª£ÁêÜIPÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        trs = soup.find('table').find('tbody').find_all('tr')
        for tr in trs:
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text
            region = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_encoding(self) -> str:
        return 'gb2312'

    def get_urls(self) -> List[str]:
        return [
            'http://www.ip3366.net/free/?stype=1',  # È´òÂåø
            'http://www.ip3366.net/free/?stype=2'  # ÈÄèÊòé or ÊôÆÂåø
        ]

    def get_page_range(self) -> Iterable:
        return range(1, 5)

    def get_page_url(self, url, page) -> str:
        return f'{url}&page={page}'

    def _judge_proxy_type(self, type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    def _judge_proxy_cover(self, cover_str: str):
        if cover_str == 'ÈÄèÊòé‰ª£ÁêÜIP':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'È´òÂåø‰ª£ÁêÜIP':
            return ProxyCoverEnum.HIGH_COVER.value
        elif cover_str == 'ÊôÆÈÄö‰ª£ÁêÜIP':
            return ProxyCoverEnum.NORMAL_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderIpHaiIp(AbsSpider):
    """
    IPÊµ∑‰ª£ÁêÜIP Âà∑Êñ∞ÈÄüÂ∫¶: 8ÂàÜÈíü/1‰∏™
    ÊúâÊó∂‰ºöËøû‰∏ç‰∏ä
    http://www.iphai.com
    """

    def __init__(self) -> None:
        super().__init__('IPÊµ∑‰ª£ÁêÜIPÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        table = soup.find('table')
        if table is None:
            return []
        tbody = soup.find('tbody')
        if tbody is None:
            return []
        trs = tbody.find_all('tr')
        for i, tr in enumerate(trs):
            if i == 0:
                continue
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text if tds[3].text != '' else 'http'
            region = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_urls(self) -> List[str]:
        return [
            'http://www.iphai.com/free/ng',  # ÂõΩÂÜÖÈ´òÂåø
            'http://www.iphai.com/free/np',  # ÂõΩÂÜÖÊôÆÈÄö
            'http://www.iphai.com/free/wg',  # ÂõΩÂ§ñÈ´òÂåø
            'http://www.iphai.com/free/wp',  # ÂõΩÂ§ñÊôÆÈÄö
        ]

    # Áà¨Â§™Âø´‰ºöË¢´Â∞Å
    def get_interval(self) -> int:
        return 2

    def get_page_url(self, url, page) -> str:
        return url

    @staticmethod
    def _judge_proxy_type(type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'ÈÄèÊòé':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'È´òÂåø':
            return ProxyCoverEnum.HIGH_COVER.value
        elif cover_str == 'ÊôÆÂåø':
            return ProxyCoverEnum.NORMAL_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderMianFeiDaiLiIp(AbsSpider):
    """
    ÂÖçË¥π‰ª£ÁêÜIPÂ∫ì
    http://ip.jiangxianli.com/
    """

    def __init__(self) -> None:
        super().__init__('ÂÖçË¥π‰ª£ÁêÜIPÁà¨Ëô´')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        table = soup.find('table')
        if table is None:
            return []
        tbody = soup.find('tbody')
        if tbody is None:
            return []
        trs = tbody.find_all('tr')
        for i, tr in enumerate(trs):
            if tr.find("div"):
                continue
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text if tds[3].text != '' else 'http'
            region = tds[5].text
            supplier = tds[6].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      supplier=supplier,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_interval(self) -> int:
        return 2

    def get_page_range(self) -> Iterable:
        return range(1, 4)

    def get_urls(self) -> List[str]:
        return ['http://ip.jiangxianli.com/?page={}']

    def get_page_url(self, url, page) -> str:
        return url.format(page)

    @staticmethod
    def _judge_proxy_type(type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'ÈÄèÊòé':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'È´òÂåø':
            return ProxyCoverEnum.HIGH_COVER.value
        elif cover_str == 'ÊôÆÂåø':
            return ProxyCoverEnum.NORMAL_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


if __name__ == '__main__':
    import time

    result = []
    import requests

    spider_obj = Spider66Ip()
    spider_obj = SpiderKuaiDaiLiIp()
    spider_obj = SpiderYunDaiLiIp()
    spider_obj = SpiderMianFeiDaiLiIp()
    for url in spider_obj.get_urls():
        for i in spider_obj.get_page_range():
            u = spider_obj.get_page_url(url, i)
            resp = requests.get(u)
            while resp.status_code != 200:
                print(u)
                time.sleep(spider_obj.get_interval())
                resp = requests.get(u)
            resp.encoding = spider_obj.get_encoding()
            temp = spider_obj.do_crawl(resp.text)
            for j in temp:
                # print(j.proxy_cover, j.region)
                if j.proxy_cover in ["1", 1]:
                    anonymity = "transparent"
                elif j.proxy_cover in ["2", 2]:
                    anonymity = "common"
                elif j.proxy_cover in ["3", 3]:
                    anonymity = "high"
                else:
                    anonymity = "unknown"

                if j.url.startswith("https://"):
                    ip, port = j.url.strip("https://").split(":")
                else:
                    ip, port = j.url.strip("http://").split(":")
                result.append({
                    "proto": "http",
                    "ip": ip,
                    "port": int(port),
                    "country": j.region,
                    "type": j.proxy_type,
                    "anonymity": anonymity,
                    "source": url,
                })
            time.sleep(spider_obj.get_interval())
    from test_proxy import main

    print(result)
    main(result)
